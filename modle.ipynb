{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Calculate and print evaluation metrics\n",
        "def evaluate(y_true, y_pred):\n",
        "    # print(f\"\\n{method_name} Metrics:\")\n",
        "    return (accuracy_score(y_true, y_pred),\n",
        "    precision_score(y_true, y_pred, average='weighted'),\n",
        "    recall_score(y_true, y_pred, average='weighted'),\n",
        "    f1_score(y_true, y_pred, average='weighted'))\n",
        "\n",
        "def compute_si(X, y):\n",
        "    \"\"\"\n",
        "    Compute the Separation Index (SI) for the given features and labels.\n",
        "    SI = (1 / m) * sum(delta(li, li*)), where li* is the label of the nearest neighbor.\n",
        "    \"\"\"\n",
        "    m = X.shape[0]  # Number of samples\n",
        "    distances = pairwise_distances(X)  # Compute pairwise distances\n",
        "    np.fill_diagonal(distances, np.inf)  # Ignore self-distances by setting them to infinity\n",
        "    nearest_neighbor_indices = np.argmin(distances, axis=1)  # Indices of nearest neighbors\n",
        "    nearest_neighbor_labels = y[nearest_neighbor_indices]  # Labels of nearest neighbors\n",
        "    delta = (nearest_neighbor_labels == y).astype(int)  # Kronecker delta\n",
        "    si = np.mean(delta)  # Compute SI\n",
        "    return si\n",
        "\n",
        "# Forward selection based on SI\n",
        "def forward_selection_si(X, y, max_features):\n",
        "    \"\"\"\n",
        "    Perform forward feature selection using Separation Index (SI).\n",
        "    \"\"\"\n",
        "    selected_features = []\n",
        "    remaining_features = list(X.columns)\n",
        "    best_si = -np.inf  # Initialize with a very low SI\n",
        "\n",
        "    while len(selected_features) < max_features and remaining_features:\n",
        "        best_candidate = None\n",
        "        best_si = -np.inf\n",
        "\n",
        "        for feature in remaining_features:\n",
        "            current_features = selected_features + [feature]\n",
        "            X_subset = X[current_features].values\n",
        "            si = compute_si(X_subset, y)\n",
        "\n",
        "            if si > best_si:\n",
        "                best_si = si\n",
        "                best_candidate = feature\n",
        "\n",
        "        if best_candidate:\n",
        "            selected_features.append(best_candidate)\n",
        "            remaining_features.remove(best_candidate)\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return selected_features"
      ],
      "metadata": {
        "id": "iNwZB10s6cAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zcxgQL1okOA"
      },
      "outputs": [],
      "source": [
        "!pip install ucimlrepo\n",
        "\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.decomposition import SparsePCA\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import pairwise_distances\n",
        "\n",
        "\n",
        "##\n",
        "\n",
        "\n",
        "# Assume that fetch_ucirepo and necessary dataset fetching utilities are defined\n",
        "# Fetch dataset\n",
        "darwin = fetch_ucirepo(id=732)\n",
        "\n",
        "# Data\n",
        "X = darwin.data.features.iloc[:, 1:]  # Removing the first feature as it is the ID of participant\n",
        "y = darwin.data.targets.iloc[:, 0]  # Assuming y is single-column\n",
        "\n",
        "# Convert categorical columns to numerical\n",
        "non_numeric_cols = X.select_dtypes(include=['object']).columns\n",
        "if len(non_numeric_cols) > 0:\n",
        "    for col in non_numeric_cols:\n",
        "        le = LabelEncoder()\n",
        "        X[col] = le.fit_transform(X[col])\n",
        "\n",
        "# Encode target\n",
        "y = LabelEncoder().fit_transform(y) if y.dtype == 'object' else y\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=6)\n",
        "\n",
        "# Data augmentation (if necessary)\n",
        "if False:\n",
        "    target_size = 500\n",
        "    current_size = X_train.shape[0]\n",
        "\n",
        "    if current_size < target_size:\n",
        "        samples_needed = target_size - current_size\n",
        "        # Sample additional data from X_train with replacement\n",
        "        additional_X = X_train.sample(n=samples_needed, replace=True, random_state=42)\n",
        "        # Sample additional labels directly from y_train\n",
        "        additional_y = np.random.choice(y_train, size=samples_needed, replace=True)\n",
        "\n",
        "        # Concatenate the original and augmented data\n",
        "        X_train = pd.concat([X_train, additional_X], ignore_index=True)\n",
        "        y_train = np.concatenate([y_train, additional_y])\n",
        "\n",
        "# Preprocessing: Low variance filtering, feature selection, and optional PCA\n",
        "# Remove low variance features\n",
        "numeric_cols = X_train.select_dtypes(include=[np.number]).columns\n",
        "variances = X_train[numeric_cols].var()\n",
        "low_variance_mask = variances > 0\n",
        "selected_numeric_cols = numeric_cols[low_variance_mask]\n",
        "\n",
        "# Ensure that non-numeric columns exist in X_train for union operation\n",
        "valid_non_numeric_cols = [col for col in non_numeric_cols if col in X_train.columns]\n",
        "\n",
        "# Filter train and test sets\n",
        "X_train_filtered = X_train[selected_numeric_cols.union(valid_non_numeric_cols)]\n",
        "X_test_filtered = X_test[selected_numeric_cols.union(valid_non_numeric_cols)]\n",
        "\n",
        "\n",
        "# Remove highly correlated features\n",
        "correlation_matrix = X_train_filtered.corr()\n",
        "correlation_threshold = 0.98\n",
        "upper_triangle = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
        "correlated_features = [column for column in upper_triangle.columns if any(upper_triangle[column] > correlation_threshold)]\n",
        "X_train_filtered = X_train_filtered.loc[:, ~X_train_filtered.columns.isin(correlated_features)]\n",
        "X_test_filtered = X_test_filtered.loc[:, ~X_test_filtered.columns.isin(correlated_features)]\n",
        "\n",
        "\n",
        "# Apply forward selection using SI\n",
        "max_features = 20  # Define the maximum number of features to select\n",
        "selected_features = forward_selection_si(X_train_filtered, y_train, max_features=max_features)\n",
        "\n",
        "# Filter train and test sets based on selected features\n",
        "X_train_SI = X_train_filtered[selected_features]\n",
        "X_test_SI = X_test_filtered[selected_features]\n",
        "\n",
        "# Feature Selection: Use SelectKBest to select top features based on ANOVA F-value\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "select_kbest = SelectKBest(score_func=f_classif, k=20)  # Select top 40 features\n",
        "X_train_ANOVA = pd.DataFrame(select_kbest.fit_transform(X_train_filtered, y_train), columns=[f'feature_{i}' for i in range(20)])\n",
        "X_test_ANOVA = pd.DataFrame(select_kbest.transform(X_test_filtered), columns=[f'feature_{i}' for i in range(20)])\n",
        "\n",
        "# Features selected by Forward Selection SI\n",
        "features_si = selected_features  # Already available from forward_selection_si\n",
        "\n",
        "# Features selected by ANOVA\n",
        "anova_selected_indices = select_kbest.get_support(indices=True)\n",
        "features_anova = X_train_filtered.columns[anova_selected_indices].tolist()\n",
        "print(f\"anova_selected_indices {anova_selected_indices}\")\n",
        "print(f\"features_anova {features_anova}\")\n",
        "\n",
        "# Compare the two methods\n",
        "features_in_both = set(features_si).intersection(features_anova)  # Features common to both methods\n",
        "features_only_in_si = set(features_si).difference(features_anova)  # Features unique to Forward Selection SI\n",
        "features_only_in_anova = set(features_anova).difference(features_si)  # Features unique to ANOVA\n",
        "\n",
        "# Display results\n",
        "print(\"Features selected by Forward Selection SI:\", features_si)\n",
        "print(\"Features selected by ANOVA:\", features_anova)\n",
        "print(\"Common features:\", features_in_both)\n",
        "print(\"Features only in Forward Selection SI:\", features_only_in_si)\n",
        "print(\"Features only in ANOVA:\", features_only_in_anova)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_SI = scaler.fit_transform(X_train_SI)\n",
        "X_test_SI = scaler.transform(X_test_SI)\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_ANOVA)\n",
        "X_test_scaled = scaler.transform(X_test_ANOVA)\n",
        "\n",
        "# Apply PCA to reduce dimensions and retain 80% of variance\n",
        "pca = PCA(n_components=0.95)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "# Concatenation Fusion: Combine original filtered features with PCA features\n",
        "X_train_concat = np.concatenate((X_train_scaled, X_train_pca), axis=1)\n",
        "X_test_concat = np.concatenate((X_test_scaled, X_test_pca), axis=1)\n",
        "\n",
        "# Sparse Representation Fusion\n",
        "sparse_pca = SparsePCA(n_components=20, random_state=42)\n",
        "X_train_sparse = sparse_pca.fit_transform(X_train_scaled)\n",
        "X_test_sparse = sparse_pca.transform(X_test_scaled)\n",
        "\n",
        "# Weighted Fusion: Assign weights to PCA and original features\n",
        "weight_pca = 0.7\n",
        "weight_original = 0.3\n",
        "X_train_weighted = np.hstack((weight_original * X_train_scaled, weight_pca * X_train_pca))\n",
        "X_test_weighted = np.hstack((weight_original * X_test_scaled, weight_pca * X_test_pca))\n",
        "\n",
        "# Prepare traditional classifiers with different fused datasets\n",
        "fused_datasets = {\n",
        "    \"SI\" : (np.array(X_train_SI), np.array(X_test_SI)),\n",
        "    \"PCA\": (X_train_pca, X_test_pca),\n",
        "    \"Concatenation\": (X_train_concat, X_test_concat),\n",
        "    \"Sparse\": (X_train_sparse, X_test_sparse),\n",
        "    \"Weighted\": (X_train_weighted, X_test_weighted)\n",
        "}\n",
        "classifiers = {}\n",
        "\n",
        "for feature_name in fused_datasets.keys():\n",
        "    classifiers.update({\n",
        "        f\"{feature_name} Random Forest\": RandomForestClassifier(random_state=42),\n",
        "        f\"{feature_name} XGBoost\": XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
        "        f\"{feature_name} Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
        "        f\"{feature_name} SVM\": SVC(probability=True, random_state=42),\n",
        "        f\"{feature_name} KNN\": KNeighborsClassifier(),\n",
        "        f\"{feature_name} Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "        f\"{feature_name} Naive Bayes\": GaussianNB(),\n",
        "        f\"{feature_name} Gradient Boost\": GradientBoostingClassifier(random_state=42),\n",
        "    })\n",
        "\n",
        "# Additional methods (majority voting, weighted voting, probability averaging)\n",
        "weights = {\n",
        "    \"Random Forest\": 2,\n",
        "    \"Logistic Regression\": 1.5,\n",
        "    \"XGBoost\": 2,\n",
        "    \"Decision Tree\": 1,\n",
        "    \"SVM\": 1,\n",
        "    \"KNN\": 1,\n",
        "    \"Naive Bayes\": 1,\n",
        "    \"Gradient Boost\": 1,\n",
        "}\n",
        "\n",
        "results = {}\n",
        "predictions = {}\n",
        "probabilities = {}\n",
        "\n",
        "for fusion_name, (X_train_fused, X_test_fused) in fused_datasets.items():\n",
        "    print(f\"Number of features for {fusion_name} fusion: {X_train_fused.shape[1]}\")\n",
        "    print(f\"SI is {compute_si(X_train_fused, y_train):.3f} for {fusion_name} features\")\n",
        "    for clf_name in weights.keys():\n",
        "        clf = classifiers[f\"{fusion_name} {clf_name}\"]\n",
        "        clf.fit(X_train_fused, y_train)\n",
        "        y_pred = clf.predict(X_test_fused)\n",
        "        predictions[clf_name] =  y_pred\n",
        "        probabilities[clf_name] = clf.predict_proba(X_test_fused)[:, 1]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Calculate various evaluation metrics\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        precision = precision_score(y_test, y_pred, average='weighted', zero_division=1)\n",
        "        recall = recall_score(y_test, y_pred, average='weighted', zero_division=1)\n",
        "        f1 = f1_score(y_test, y_pred, average='weighted', zero_division=1)\n",
        "\n",
        "        # Store metrics in the results dictionary\n",
        "        results[f\"{fusion_name} + {clf_name}\"] = {\n",
        "            \"accuracy\": accuracy,\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"f1\": f1\n",
        "        }\n",
        "\n",
        "        # Identify and print misclassified samples\n",
        "        misclassified_indices = np.where(y_test != y_pred)[0]\n",
        "        if len(misclassified_indices) > 0:\n",
        "            print(f\"\\nMisclassified samples for {fusion_name} + {clf_name}:\")\n",
        "            for idx in misclassified_indices:\n",
        "                print(f\"  Index: {idx}, True Label: {y_test[idx]}, Predicted Label: {y_pred[idx]}\")\n",
        "    pred_df = pd.DataFrame(predictions)\n",
        "\n",
        "    weighted_votes = np.zeros(len(X_test_fused))\n",
        "\n",
        "    for name, weight in weights.items():\n",
        "        weighted_votes += pred_df[name] * weight\n",
        "    majority_vote_pred = pred_df.mode(axis=1)[0]  # Most common prediction for each instance\n",
        "\n",
        "    weighted_vote_pred = (weighted_votes >= (sum(weights.values()) / 2)).astype(int)\n",
        "    avg_prob = sum(weight * probabilities[name] for name, weight in weights.items()) / sum(weights.values())\n",
        "    prob_avg_pred = (avg_prob >= 0.5).astype(int)\n",
        "    accuracy, precision, recall,f1 =  evaluate(y_test, majority_vote_pred)\n",
        "    results[f\"{fusion_name} + Majority Voting\"] = {\n",
        "            \"accuracy\": accuracy,\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"f1\": f1\n",
        "    }\n",
        "    accuracy, precision, recall,f1 =  evaluate(y_test, weighted_vote_pred)\n",
        "    results[f\"{fusion_name} + Weighted Voting\"] = {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1\n",
        "    }\n",
        "    accuracy, precision, recall,f1 =  evaluate(y_test, prob_avg_pred)\n",
        "    results[f\"{fusion_name} + Probability Voting\"] = {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1\n",
        "    }\n",
        "y_predict_MCF= np.zeros((y_test.shape[0],len(fused_datasets)))\n",
        "j = 0\n",
        "for fusion_name, (X_train_fused, X_test_fused) in fused_datasets.items():\n",
        "\n",
        "    # Define and train a simple MLP model\n",
        "    mlp_model = Sequential()\n",
        "    mlp_model.add(Dense(4, input_dim=X_train_fused.shape[1], activation='relu'))\n",
        "    mlp_model.add(Dense(7, activation='relu'))\n",
        "    mlp_model.add(Dense(len(weights), activation='softmax'))\n",
        "\n",
        "    mlp_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    # Define number of steps and folds for the grid layout\n",
        "    num_steps = 11\n",
        "    num_folds = 12\n",
        "\n",
        "    # # Initialize figure with a 5x5 grid for subplots\n",
        "    # fig, axs = plt.subplots(num_steps, num_folds, figsize=(20, 20))\n",
        "    # fig.suptitle('Model Accuracy and Loss Across Steps and Folds', fontsize=16)\n",
        "\n",
        "    # Set up K-Fold cross-validation and list to store history data\n",
        "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=6)\n",
        "    # Initialize lists to store mean metrics across folds for each step\n",
        "    mean_train_accuracy_per_step = []\n",
        "    mean_val_accuracy_per_step = []\n",
        "    mean_train_loss_per_step = []\n",
        "    mean_val_loss_per_step = []\n",
        "    for step in range(num_steps):\n",
        "        train_accuracy_per_fold = []\n",
        "        val_accuracy_per_fold = []\n",
        "        train_loss_per_fold = []\n",
        "        val_loss_per_fold = []\n",
        "        for fold, (train_index, test_index) in enumerate(kf.split(X_train_fused)):\n",
        "            print(f\"\\nProcessing Fold {fold + 1}\")\n",
        "\n",
        "            # Split data into train and test sets for the current fold\n",
        "            _X_train, X_val = X_train_fused[train_index], X_train_fused[test_index]\n",
        "            y_train_mlp, y_val = y_train[train_index], y_train[test_index]\n",
        "\n",
        "            _classifiers ={\n",
        "                \"Random Forest\": RandomForestClassifier(random_state=42),\n",
        "                \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "                \"XGBoost\": XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
        "                \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
        "                \"SVM\": SVC(probability=True, random_state=42),\n",
        "                \"KNN\": KNeighborsClassifier(),\n",
        "                \"Naive Bayes\": GaussianNB(),\n",
        "                \"Gradient Boost\": GradientBoostingClassifier(random_state=42),\n",
        "            }\n",
        "\n",
        "            # Train and evaluate each classifier\n",
        "            for clf_name, clf in _classifiers.items():\n",
        "                clf.fit(_X_train, y_train_mlp)\n",
        "                # y_pred = clf.predict_proba(X_test_fused)\n",
        "            y_for_mlp_val = np.zeros((X_val.shape[0],len(_classifiers)))\n",
        "            # y_for_mlp_train = np.zeros((_X_train.shape[0],len(_classifiers)))\n",
        "\n",
        "            y_for_mlp_test = np.zeros((X_test_fused.shape[0],len(_classifiers)))\n",
        "            for idx,_x in enumerate(X_val):\n",
        "                if y_val[idx] == 1:\n",
        "                    _y = np.array([clf.predict_proba(_x.reshape(1,-1))*(results[f\"{fusion_name} + {_}\"][\"f1\"])**4 for _,clf in _classifiers.items()])[:,0,1]\n",
        "                else:\n",
        "                    _y = np.array([clf.predict_proba(_x.reshape(1,-1))*(results[f\"{fusion_name} + {_}\"][\"f1\"])**4 for _,clf in _classifiers.items()])[:,0,0]\n",
        "                y_for_mlp_val[idx,np.argmax(_y)] = 1\n",
        "                print(_y)\n",
        "            y_for_mlp_val = np.argmax(y_for_mlp_val,axis=1)\n",
        "\n",
        "\n",
        "            for idx,_x in enumerate(X_test_fused):\n",
        "                if y_test[idx] == 1:\n",
        "                    _y = np.array([clf.predict_proba(_x.reshape(1,-1))*(results[f\"{fusion_name} + {_}\"][\"f1\"])**4 for _,clf in _classifiers.items()])[:,0,1]\n",
        "                else:\n",
        "                    _y = np.array([clf.predict_proba(_x.reshape(1,-1))*(results[f\"{fusion_name} + {_}\"][\"f1\"])**4 for _,clf in _classifiers.items()])[:,0,0]\n",
        "                y_for_mlp_test[idx,np.argmax(_y)] = 1\n",
        "            y_for_mlp_test = np.argmax(y_for_mlp_test,axis=1)\n",
        "            print(y_for_mlp_val)\n",
        "\n",
        "            history = mlp_model.fit(X_val , y_for_mlp_val, epochs=15, batch_size=16, verbose=1, validation_data=(X_test_fused, y_for_mlp_test))\n",
        "\n",
        "            # Capture the final train and validation accuracy and loss for each fold in this step\n",
        "            train_accuracy_per_fold.append(history.history['accuracy'][-1])\n",
        "            val_accuracy_per_fold.append(history.history['val_accuracy'][-1])\n",
        "            train_loss_per_fold.append(history.history['loss'][-1])\n",
        "            val_loss_per_fold.append(history.history['val_loss'][-1])\n",
        "\n",
        "        # Calculate the mean metrics across folds for this step\n",
        "        mean_train_accuracy_per_step.append(np.mean(train_accuracy_per_fold))\n",
        "        mean_val_accuracy_per_step.append(np.mean(val_accuracy_per_fold))\n",
        "        mean_train_loss_per_step.append(np.mean(train_loss_per_fold))\n",
        "        mean_val_loss_per_step.append(np.mean(val_loss_per_fold))\n",
        "\n",
        "    # Plot mean accuracy over steps\n",
        "    plt.figure(figsize=(10, 5))\n",
        "\n",
        "    # Plot training and validation accuracies\n",
        "    plt.plot(range(1, num_steps + 1), mean_train_accuracy_per_step, marker='o', linestyle='-', label='Mean Train Accuracy')\n",
        "    plt.plot(range(1, num_steps + 1), mean_val_accuracy_per_step, marker='o', linestyle='--', label='Mean Validation Accuracy')\n",
        "    plt.title('Mean Training and Validation Accuracy Across Steps')\n",
        "    plt.xlabel('Step')\n",
        "    plt.ylabel('Mean Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot mean loss over steps\n",
        "    plt.figure(figsize=(10, 5))\n",
        "\n",
        "    # Plot training and validation losses\n",
        "    plt.plot(range(1, num_steps + 1), mean_train_loss_per_step, marker='o', linestyle='-', label='Mean Train Loss')\n",
        "    plt.plot(range(1, num_steps + 1), mean_val_loss_per_step, marker='o', linestyle='--', label='Mean Validation Loss')\n",
        "    plt.title('Mean Training and Validation Loss Across Steps')\n",
        "    plt.xlabel('Step')\n",
        "    plt.ylabel('Mean Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    selected_classifier = np.argmax(mlp_model.predict(X_test_fused), axis=1)\n",
        "    _y_pred = np.zeros((y_test.shape[0],len(_classifiers)))\n",
        "\n",
        "    for i, clf_name in enumerate(weights.keys()):\n",
        "        clf = classifiers[f\"{fusion_name} {clf_name}\"]\n",
        "        _y_pred[:,i] = clf.predict(X_test_fused)\n",
        "    # y_pred = _y_pred[:,selected_classifier]\n",
        "    y_pred = _y_pred[np.arange(_y_pred.shape[0]), selected_classifier]\n",
        "    y_predict_MCF[:,j] = y_pred\n",
        "    j += 1\n",
        "    # Calculate various evaluation metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=1)\n",
        "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=1)\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=1)\n",
        "\n",
        "    # Store metrics in the results dictionary\n",
        "    results[f\"{fusion_name} + MCF\"] = {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1\n",
        "    }\n",
        "    print(results[f\"{fusion_name} + MCF\"])\n",
        "    # Identify and print misclassified samples\n",
        "    misclassified_indices = np.where(y_test != y_pred)[0]\n",
        "    if len(misclassified_indices) > 0:\n",
        "        print(f\"\\nMisclassified samples for {fusion_name} + MCF:\")\n",
        "        for idx in misclassified_indices:\n",
        "            print(f\"  Index: {idx}, True Label: {y_test[idx]}, Predicted Label: {y_pred[idx]}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ucimlrepo\n",
        "\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.decomposition import SparsePCA\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import pairwise_distances\n",
        "\n",
        "\n",
        "##\n",
        "\n",
        "\n",
        "# Assume that fetch_ucirepo and necessary dataset fetching utilities are defined\n",
        "# Fetch dataset\n",
        "darwin = fetch_ucirepo(id=732)\n",
        "\n",
        "# Data\n",
        "X = darwin.data.features.iloc[:, 1:]  # Removing the first feature as it is the ID of participant\n",
        "y = darwin.data.targets.iloc[:, 0]  # Assuming y is single-column\n",
        "\n",
        "# Convert categorical columns to numerical\n",
        "non_numeric_cols = X.select_dtypes(include=['object']).columns\n",
        "if len(non_numeric_cols) > 0:\n",
        "    for col in non_numeric_cols:\n",
        "        le = LabelEncoder()\n",
        "        X[col] = le.fit_transform(X[col])\n",
        "\n",
        "# Encode target\n",
        "y = LabelEncoder().fit_transform(y) if y.dtype == 'object' else y\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=6)\n",
        "\n",
        "# Data augmentation (if necessary)\n",
        "if False:\n",
        "    target_size = 500\n",
        "    current_size = X_train.shape[0]\n",
        "\n",
        "    if current_size < target_size:\n",
        "        samples_needed = target_size - current_size\n",
        "        # Sample additional data from X_train with replacement\n",
        "        additional_X = X_train.sample(n=samples_needed, replace=True, random_state=42)\n",
        "        # Sample additional labels directly from y_train\n",
        "        additional_y = np.random.choice(y_train, size=samples_needed, replace=True)\n",
        "\n",
        "        # Concatenate the original and augmented data\n",
        "        X_train = pd.concat([X_train, additional_X], ignore_index=True)\n",
        "        y_train = np.concatenate([y_train, additional_y])\n",
        "\n",
        "# Preprocessing: Low variance filtering, feature selection, and optional PCA\n",
        "# Remove low variance features\n",
        "numeric_cols = X_train.select_dtypes(include=[np.number]).columns\n",
        "variances = X_train[numeric_cols].var()\n",
        "low_variance_mask = variances > 0\n",
        "selected_numeric_cols = numeric_cols[low_variance_mask]\n",
        "\n",
        "# Ensure that non-numeric columns exist in X_train for union operation\n",
        "valid_non_numeric_cols = [col for col in non_numeric_cols if col in X_train.columns]\n",
        "\n",
        "# Filter train and test sets\n",
        "X_train_filtered = X_train[selected_numeric_cols.union(valid_non_numeric_cols)]\n",
        "X_test_filtered = X_test[selected_numeric_cols.union(valid_non_numeric_cols)]\n",
        "\n",
        "\n",
        "# Remove highly correlated features\n",
        "correlation_matrix = X_train_filtered.corr()\n",
        "correlation_threshold = 0.98\n",
        "upper_triangle = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
        "correlated_features = [column for column in upper_triangle.columns if any(upper_triangle[column] > correlation_threshold)]\n",
        "X_train_filtered = X_train_filtered.loc[:, ~X_train_filtered.columns.isin(correlated_features)]\n",
        "X_test_filtered = X_test_filtered.loc[:, ~X_test_filtered.columns.isin(correlated_features)]\n",
        "\n",
        "\n",
        "# Apply forward selection using SI\n",
        "max_features = 20  # Define the maximum number of features to select\n",
        "selected_features = forward_selection_si(X_train_filtered, y_train, max_features=max_features)\n",
        "\n",
        "# Filter train and test sets based on selected features\n",
        "X_train_SI = X_train_filtered[selected_features]\n",
        "X_test_SI = X_test_filtered[selected_features]\n",
        "\n",
        "# Feature Selection: Use SelectKBest to select top features based on ANOVA F-value\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "select_kbest = SelectKBest(score_func=f_classif, k=20)  # Select top 40 features\n",
        "X_train_ANOVA = pd.DataFrame(select_kbest.fit_transform(X_train_filtered, y_train), columns=[f'feature_{i}' for i in range(20)])\n",
        "X_test_ANOVA = pd.DataFrame(select_kbest.transform(X_test_filtered), columns=[f'feature_{i}' for i in range(20)])\n",
        "\n",
        "# Features selected by Forward Selection SI\n",
        "features_si = selected_features  # Already available from forward_selection_si\n",
        "\n",
        "# Features selected by ANOVA\n",
        "anova_selected_indices = select_kbest.get_support(indices=True)\n",
        "features_anova = X_train_filtered.columns[anova_selected_indices].tolist()\n",
        "print(f\"anova_selected_indices {anova_selected_indices}\")\n",
        "print(f\"features_anova {features_anova}\")\n",
        "\n",
        "# Compare the two methods\n",
        "features_in_both = set(features_si).intersection(features_anova)  # Features common to both methods\n",
        "features_only_in_si = set(features_si).difference(features_anova)  # Features unique to Forward Selection SI\n",
        "features_only_in_anova = set(features_anova).difference(features_si)  # Features unique to ANOVA\n",
        "\n",
        "# Display results\n",
        "print(\"Features selected by Forward Selection SI:\", features_si)\n",
        "print(\"Features selected by ANOVA:\", features_anova)\n",
        "print(\"Common features:\", features_in_both)\n",
        "print(\"Features only in Forward Selection SI:\", features_only_in_si)\n",
        "print(\"Features only in ANOVA:\", features_only_in_anova)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_SI = scaler.fit_transform(X_train_SI)\n",
        "X_test_SI = scaler.transform(X_test_SI)\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_ANOVA)\n",
        "X_test_scaled = scaler.transform(X_test_ANOVA)\n",
        "\n",
        "# Apply PCA to reduce dimensions and retain 80% of variance\n",
        "pca = PCA(n_components=0.95)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "# Concatenation Fusion: Combine original filtered features with PCA features\n",
        "X_train_concat = np.concatenate((X_train_scaled, X_train_pca), axis=1)\n",
        "X_test_concat = np.concatenate((X_test_scaled, X_test_pca), axis=1)\n",
        "\n",
        "# Sparse Representation Fusion\n",
        "sparse_pca = SparsePCA(n_components=20, random_state=42)\n",
        "X_train_sparse = sparse_pca.fit_transform(X_train_scaled)\n",
        "X_test_sparse = sparse_pca.transform(X_test_scaled)\n",
        "\n",
        "# Weighted Fusion: Assign weights to PCA and original features\n",
        "weight_pca = 0.7\n",
        "weight_original = 0.3\n",
        "X_train_weighted = np.hstack((weight_original * X_train_scaled, weight_pca * X_train_pca))\n",
        "X_test_weighted = np.hstack((weight_original * X_test_scaled, weight_pca * X_test_pca))\n",
        "\n",
        "# Prepare traditional classifiers with different fused datasets\n",
        "fused_datasets = {\n",
        "    \"SI\" : (np.array(X_train_SI), np.array(X_test_SI)),\n",
        "    \"PCA\": (X_train_pca, X_test_pca),\n",
        "    \"Concatenation\": (X_train_concat, X_test_concat),\n",
        "    \"Sparse\": (X_train_sparse, X_test_sparse),\n",
        "    \"Weighted\": (X_train_weighted, X_test_weighted)\n",
        "}\n",
        "classifiers = {}\n",
        "\n",
        "for feature_name in fused_datasets.keys():\n",
        "    classifiers.update({\n",
        "        f\"{feature_name} Random Forest\": RandomForestClassifier(random_state=42),\n",
        "        f\"{feature_name} XGBoost\": XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
        "        f\"{feature_name} Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
        "        f\"{feature_name} SVM\": SVC(probability=True, random_state=42),\n",
        "        f\"{feature_name} KNN\": KNeighborsClassifier(),\n",
        "        f\"{feature_name} Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "        f\"{feature_name} Naive Bayes\": GaussianNB(),\n",
        "        f\"{feature_name} Gradient Boost\": GradientBoostingClassifier(random_state=42),\n",
        "    })\n",
        "\n",
        "# Additional methods (majority voting, weighted voting, probability averaging)\n",
        "weights = {\n",
        "    \"Random Forest\": 2,\n",
        "    \"Logistic Regression\": 1.5,\n",
        "    \"XGBoost\": 2,\n",
        "    \"Decision Tree\": 1,\n",
        "    \"SVM\": 1,\n",
        "    \"KNN\": 1,\n",
        "    \"Naive Bayes\": 1,\n",
        "    \"Gradient Boost\": 1,\n",
        "}\n",
        "\n",
        "results = {}\n",
        "predictions = {}\n",
        "probabilities = {}\n",
        "\n",
        "for fusion_name, (X_train_fused, X_test_fused) in fused_datasets.items():\n",
        "    print(f\"Number of features for {fusion_name} fusion: {X_train_fused.shape[1]}\")\n",
        "    print(f\"SI is {compute_si(X_train_fused, y_train):.3f} for {fusion_name} features\")\n",
        "    for clf_name in weights.keys():\n",
        "        clf = classifiers[f\"{fusion_name} {clf_name}\"]\n",
        "        clf.fit(X_train_fused, y_train)\n",
        "        y_pred = clf.predict(X_test_fused)\n",
        "        predictions[clf_name] =  y_pred\n",
        "        probabilities[clf_name] = clf.predict_proba(X_test_fused)[:, 1]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Calculate various evaluation metrics\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        precision = precision_score(y_test, y_pred, average='weighted', zero_division=1)\n",
        "        recall = recall_score(y_test, y_pred, average='weighted', zero_division=1)\n",
        "        f1 = f1_score(y_test, y_pred, average='weighted', zero_division=1)\n",
        "\n",
        "        # Store metrics in the results dictionary\n",
        "        results[f\"{fusion_name} + {clf_name}\"] = {\n",
        "            \"accuracy\": accuracy,\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"f1\": f1\n",
        "        }\n",
        "\n",
        "        # Identify and print misclassified samples\n",
        "        misclassified_indices = np.where(y_test != y_pred)[0]\n",
        "        if len(misclassified_indices) > 0:\n",
        "            print(f\"\\nMisclassified samples for {fusion_name} + {clf_name}:\")\n",
        "            for idx in misclassified_indices:\n",
        "                print(f\"  Index: {idx}, True Label: {y_test[idx]}, Predicted Label: {y_pred[idx]}\")\n",
        "    pred_df = pd.DataFrame(predictions)\n",
        "\n",
        "    weighted_votes = np.zeros(len(X_test_fused))\n",
        "\n",
        "    for name, weight in weights.items():\n",
        "        weighted_votes += pred_df[name] * weight\n",
        "    majority_vote_pred = pred_df.mode(axis=1)[0]  # Most common prediction for each instance\n",
        "\n",
        "    weighted_vote_pred = (weighted_votes >= (sum(weights.values()) / 2)).astype(int)\n",
        "    avg_prob = sum(weight * probabilities[name] for name, weight in weights.items()) / sum(weights.values())\n",
        "    prob_avg_pred = (avg_prob >= 0.5).astype(int)\n",
        "    accuracy, precision, recall,f1 =  evaluate(y_test, majority_vote_pred)\n",
        "    results[f\"{fusion_name} + Majority Voting\"] = {\n",
        "            \"accuracy\": accuracy,\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"f1\": f1\n",
        "    }\n",
        "    accuracy, precision, recall,f1 =  evaluate(y_test, weighted_vote_pred)\n",
        "    results[f\"{fusion_name} + Weighted Voting\"] = {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1\n",
        "    }\n",
        "    accuracy, precision, recall,f1 =  evaluate(y_test, prob_avg_pred)\n",
        "    results[f\"{fusion_name} + Probability Voting\"] = {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1\n",
        "    }\n",
        "y_predict_MCF= np.zeros((y_test.shape[0],len(fused_datasets)))\n",
        "j = 0\n",
        "for fusion_name, (X_train_fused, X_test_fused) in fused_datasets.items():\n",
        "\n",
        "    # Define and train a simple MLP model\n",
        "    mlp_model = Sequential()\n",
        "    mlp_model.add(Dense(4, input_dim=X_train_fused.shape[1], activation='relu'))\n",
        "    mlp_model.add(Dense(5, activation='relu'))\n",
        "    mlp_model.add(Dense(len(weights), activation='softmax'))\n",
        "\n",
        "    mlp_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    # Define number of steps and folds for the grid layout\n",
        "    num_steps = 7\n",
        "    num_folds = 12\n",
        "\n",
        "    # # Initialize figure with a 5x5 grid for subplots\n",
        "    # fig, axs = plt.subplots(num_steps, num_folds, figsize=(20, 20))\n",
        "    # fig.suptitle('Model Accuracy and Loss Across Steps and Folds', fontsize=16)\n",
        "\n",
        "    # Set up K-Fold cross-validation and list to store history data\n",
        "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=6)\n",
        "    # Initialize lists to store mean metrics across folds for each step\n",
        "    mean_train_accuracy_per_step = []\n",
        "    mean_val_accuracy_per_step = []\n",
        "    mean_train_loss_per_step = []\n",
        "    mean_val_loss_per_step = []\n",
        "    for step in range(num_steps):\n",
        "        train_accuracy_per_fold = []\n",
        "        val_accuracy_per_fold = []\n",
        "        train_loss_per_fold = []\n",
        "        val_loss_per_fold = []\n",
        "        for fold, (train_index, test_index) in enumerate(kf.split(X_train_fused)):\n",
        "            print(f\"\\nProcessing Fold {fold + 1}\")\n",
        "\n",
        "            # Split data into train and test sets for the current fold\n",
        "            _X_train, X_val = X_train_fused[train_index], X_train_fused[test_index]\n",
        "            y_train_mlp, y_val = y_train[train_index], y_train[test_index]\n",
        "\n",
        "            _classifiers ={\n",
        "                \"Random Forest\": RandomForestClassifier(random_state=42),\n",
        "                \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "                \"XGBoost\": XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
        "                \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
        "                \"SVM\": SVC(probability=True, random_state=42),\n",
        "                \"KNN\": KNeighborsClassifier(),\n",
        "                \"Naive Bayes\": GaussianNB(),\n",
        "                \"Gradient Boost\": GradientBoostingClassifier(random_state=42),\n",
        "            }\n",
        "\n",
        "            # Train and evaluate each classifier\n",
        "            for clf_name, clf in _classifiers.items():\n",
        "                clf.fit(_X_train, y_train_mlp)\n",
        "                # y_pred = clf.predict_proba(X_test_fused)\n",
        "            y_for_mlp_val = np.zeros((X_val.shape[0],len(_classifiers)))\n",
        "            # y_for_mlp_train = np.zeros((_X_train.shape[0],len(_classifiers)))\n",
        "\n",
        "            y_for_mlp_test = np.zeros((X_test_fused.shape[0],len(_classifiers)))\n",
        "            for idx,_x in enumerate(X_val):\n",
        "                if y_val[idx] == 1:\n",
        "                    _y = np.array([clf.predict_proba(_x.reshape(1,-1))*(results[f\"{fusion_name} + {_}\"][\"f1\"])**4 for _,clf in _classifiers.items()])[:,0,1]\n",
        "                else:\n",
        "                    _y = np.array([clf.predict_proba(_x.reshape(1,-1))*(results[f\"{fusion_name} + {_}\"][\"f1\"])**4 for _,clf in _classifiers.items()])[:,0,0]\n",
        "                y_for_mlp_val[idx,np.argmax(_y)] = 1\n",
        "                print(_y)\n",
        "            y_for_mlp_val = np.argmax(y_for_mlp_val,axis=1)\n",
        "\n",
        "\n",
        "            for idx,_x in enumerate(X_test_fused):\n",
        "                if y_test[idx] == 1:\n",
        "                    _y = np.array([clf.predict_proba(_x.reshape(1,-1))*(results[f\"{fusion_name} + {_}\"][\"f1\"])**4 for _,clf in _classifiers.items()])[:,0,1]\n",
        "                else:\n",
        "                    _y = np.array([clf.predict_proba(_x.reshape(1,-1))*(results[f\"{fusion_name} + {_}\"][\"f1\"])**4 for _,clf in _classifiers.items()])[:,0,0]\n",
        "                y_for_mlp_test[idx,np.argmax(_y)] = 1\n",
        "            y_for_mlp_test = np.argmax(y_for_mlp_test,axis=1)\n",
        "            print(y_for_mlp_val)\n",
        "\n",
        "            history = mlp_model.fit(X_val , y_for_mlp_val, epochs=15, batch_size=16, verbose=1, validation_data=(X_test_fused, y_for_mlp_test))\n",
        "\n",
        "            # Capture the final train and validation accuracy and loss for each fold in this step\n",
        "            train_accuracy_per_fold.append(history.history['accuracy'][-1])\n",
        "            val_accuracy_per_fold.append(history.history['val_accuracy'][-1])\n",
        "            train_loss_per_fold.append(history.history['loss'][-1])\n",
        "            val_loss_per_fold.append(history.history['val_loss'][-1])\n",
        "\n",
        "        # Calculate the mean metrics across folds for this step\n",
        "        mean_train_accuracy_per_step.append(np.mean(train_accuracy_per_fold))\n",
        "        mean_val_accuracy_per_step.append(np.mean(val_accuracy_per_fold))\n",
        "        mean_train_loss_per_step.append(np.mean(train_loss_per_fold))\n",
        "        mean_val_loss_per_step.append(np.mean(val_loss_per_fold))\n",
        "\n",
        "    # Plot mean accuracy over steps\n",
        "    plt.figure(figsize=(10, 5))\n",
        "\n",
        "    # Plot training and validation accuracies\n",
        "    plt.plot(range(1, num_steps + 1), mean_train_accuracy_per_step, marker='o', linestyle='-', label='Mean Train Accuracy')\n",
        "    plt.plot(range(1, num_steps + 1), mean_val_accuracy_per_step, marker='o', linestyle='--', label='Mean Validation Accuracy')\n",
        "    plt.title('Mean Training and Validation Accuracy Across Steps')\n",
        "    plt.xlabel('Step')\n",
        "    plt.ylabel('Mean Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot mean loss over steps\n",
        "    plt.figure(figsize=(10, 5))\n",
        "\n",
        "    # Plot training and validation losses\n",
        "    plt.plot(range(1, num_steps + 1), mean_train_loss_per_step, marker='o', linestyle='-', label='Mean Train Loss')\n",
        "    plt.plot(range(1, num_steps + 1), mean_val_loss_per_step, marker='o', linestyle='--', label='Mean Validation Loss')\n",
        "    plt.title('Mean Training and Validation Loss Across Steps')\n",
        "    plt.xlabel('Step')\n",
        "    plt.ylabel('Mean Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    selected_classifier = np.argmax(mlp_model.predict(X_test_fused), axis=1)\n",
        "    _y_pred = np.zeros((y_test.shape[0],len(_classifiers)))\n",
        "\n",
        "    for i, clf_name in enumerate(weights.keys()):\n",
        "        clf = classifiers[f\"{fusion_name} {clf_name}\"]\n",
        "        _y_pred[:,i] = clf.predict(X_test_fused)\n",
        "    # y_pred = _y_pred[:,selected_classifier]\n",
        "    y_pred = _y_pred[np.arange(_y_pred.shape[0]), selected_classifier]\n",
        "    y_predict_MCF[:,j] = y_pred\n",
        "    j += 1\n",
        "    # Calculate various evaluation metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=1)\n",
        "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=1)\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=1)\n",
        "\n",
        "    # Store metrics in the results dictionary\n",
        "    results[f\"{fusion_name} + MCF\"] = {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1\n",
        "    }\n",
        "    print(results[f\"{fusion_name} + MCF\"])\n",
        "    # Identify and print misclassified samples\n",
        "    misclassified_indices = np.where(y_test != y_pred)[0]\n",
        "    if len(misclassified_indices) > 0:\n",
        "        print(f\"\\nMisclassified samples for {fusion_name} + MCF:\")\n",
        "        for idx in misclassified_indices:\n",
        "            print(f\"  Index: {idx}, True Label: {y_test[idx]}, Predicted Label: {y_pred[idx]}\")"
      ],
      "metadata": {
        "id": "M2tEsvtyJagZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ucimlrepo\n",
        "\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.decomposition import SparsePCA\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "##\n",
        "\n",
        "\n",
        "# Assume that fetch_ucirepo and necessary dataset fetching utilities are defined\n",
        "# Fetch dataset\n",
        "darwin = fetch_ucirepo(id=732)\n",
        "\n",
        "# Data\n",
        "X = darwin.data.features.iloc[:, 1:]  # Removing the first feature as it is the ID of participant\n",
        "y = darwin.data.targets.iloc[:, 0]  # Assuming y is single-column\n",
        "\n",
        "# Convert categorical columns to numerical\n",
        "non_numeric_cols = X.select_dtypes(include=['object']).columns\n",
        "if len(non_numeric_cols) > 0:\n",
        "    for col in non_numeric_cols:\n",
        "        le = LabelEncoder()\n",
        "        X[col] = le.fit_transform(X[col])\n",
        "\n",
        "# Encode target\n",
        "y = LabelEncoder().fit_transform(y) if y.dtype == 'object' else y\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=6)\n",
        "\n",
        "# Data augmentation (if necessary)\n",
        "if False:\n",
        "    target_size = 500\n",
        "    current_size = X_train.shape[0]\n",
        "\n",
        "    if current_size < target_size:\n",
        "        samples_needed = target_size - current_size\n",
        "        # Sample additional data from X_train with replacement\n",
        "        additional_X = X_train.sample(n=samples_needed, replace=True, random_state=42)\n",
        "        # Sample additional labels directly from y_train\n",
        "        additional_y = np.random.choice(y_train, size=samples_needed, replace=True)\n",
        "\n",
        "        # Concatenate the original and augmented data\n",
        "        X_train = pd.concat([X_train, additional_X], ignore_index=True)\n",
        "        y_train = np.concatenate([y_train, additional_y])\n",
        "\n",
        "# Preprocessing: Low variance filtering, feature selection, and optional PCA\n",
        "# Remove low variance features\n",
        "numeric_cols = X_train.select_dtypes(include=[np.number]).columns\n",
        "variances = X_train[numeric_cols].var()\n",
        "low_variance_mask = variances > 0\n",
        "selected_numeric_cols = numeric_cols[low_variance_mask]\n",
        "\n",
        "# Ensure that non-numeric columns exist in X_train for union operation\n",
        "valid_non_numeric_cols = [col for col in non_numeric_cols if col in X_train.columns]\n",
        "\n",
        "# Filter train and test sets\n",
        "X_train_filtered = X_train[selected_numeric_cols.union(valid_non_numeric_cols)]\n",
        "X_test_filtered = X_test[selected_numeric_cols.union(valid_non_numeric_cols)]\n",
        "\n",
        "print(\"X_train_filtered shape:\", X_train_filtered.shape)\n",
        "print(\"X_test_filtered shape:\", X_test_filtered.shape)\n",
        "\n",
        "\n",
        "\n",
        "# Remove highly correlated features\n",
        "correlation_matrix = X_train_filtered.corr()\n",
        "correlation_threshold = 0.80\n",
        "upper_triangle = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
        "correlated_features = [column for column in upper_triangle.columns if any(upper_triangle[column] > correlation_threshold)]\n",
        "X_train_filtered = X_train_filtered.loc[:, ~X_train_filtered.columns.isin(correlated_features)]\n",
        "X_test_filtered = X_test_filtered.loc[:, ~X_test_filtered.columns.isin(correlated_features)]\n",
        "\n",
        "# Feature Selection: Use SelectKBest to select top features based on ANOVA F-value\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "select_kbest = SelectKBest(score_func=f_classif, k=20)  # Select top 20 features\n",
        "X_train_filtered = pd.DataFrame(select_kbest.fit_transform(X_train_filtered, y_train), columns=[f'feature_{i}' for i in range(20)])\n",
        "X_test_filtered = pd.DataFrame(select_kbest.transform(X_test_filtered), columns=[f'feature_{i}' for i in range(20)])\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_filtered)\n",
        "X_test_scaled = scaler.transform(X_test_filtered)\n",
        "\n",
        "# Apply PCA to reduce dimensions and retain 80% of variance\n",
        "pca = PCA(n_components=0.80)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "# Concatenation Fusion: Combine original filtered features with PCA features\n",
        "X_train_concat = np.concatenate((X_train_scaled, X_train_pca), axis=1)\n",
        "X_test_concat = np.concatenate((X_test_scaled, X_test_pca), axis=1)\n",
        "\n",
        "# Sparse Representation Fusion\n",
        "sparse_pca = SparsePCA(n_components=10, random_state=42)\n",
        "X_train_sparse = sparse_pca.fit_transform(X_train_scaled)\n",
        "X_test_sparse = sparse_pca.transform(X_test_scaled)\n",
        "\n",
        "# Weighted Fusion: Assign weights to PCA and original features\n",
        "weight_pca = 0.7\n",
        "weight_original = 0.3\n",
        "X_train_weighted = np.hstack((weight_original * X_train_scaled, weight_pca * X_train_pca))\n",
        "X_test_weighted = np.hstack((weight_original * X_test_scaled, weight_pca * X_test_pca))\n",
        "\n",
        "# Prepare traditional classifiers with different fused datasets\n",
        "fused_datasets = {\n",
        "    \"PCA\": (X_train_pca, X_test_pca),\n",
        "    \"Concatenation\": (X_train_concat, X_test_concat),\n",
        "    \"Sparse\": (X_train_sparse, X_test_sparse),\n",
        "    \"Weighted\": (X_train_weighted, X_test_weighted)\n",
        "}\n",
        "classifiers = {}\n",
        "\n",
        "for feature_name in fused_datasets.keys():\n",
        "    classifiers.update({\n",
        "        f\"{feature_name} Random Forest\": RandomForestClassifier(random_state=42),\n",
        "        f\"{feature_name} XGBoost\": XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
        "        f\"{feature_name} Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
        "        f\"{feature_name} SVM\": SVC(probability=True, random_state=42),\n",
        "        f\"{feature_name} KNN\": KNeighborsClassifier(),\n",
        "        f\"{feature_name} Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "        f\"{feature_name} Naive Bayes\": GaussianNB(),\n",
        "        f\"{feature_name} Gradient Boost\": GradientBoostingClassifier(random_state=42),\n",
        "    })\n",
        "\n",
        "# Additional methods (majority voting, weighted voting, probability averaging)\n",
        "weights = {\n",
        "    \"Random Forest\": 2,\n",
        "    \"Logistic Regression\": 1.5,\n",
        "    \"XGBoost\": 2,\n",
        "    \"Decision Tree\": 1,\n",
        "    \"SVM\": 1,\n",
        "    \"KNN\": 1,\n",
        "    \"Naive Bayes\": 1,\n",
        "    \"Gradient Boost\": 1,\n",
        "}\n",
        "\n",
        "results = {}\n",
        "predictions = {}\n",
        "probabilities = {}\n",
        "\n",
        "for fusion_name, (X_train_fused, X_test_fused) in fused_datasets.items():\n",
        "    print(f\"Number of features for {fusion_name} fusion: {X_train_fused.shape[1]}\")\n",
        "\n",
        "    for clf_name in weights.keys():\n",
        "        clf = classifiers[f\"{fusion_name} {clf_name}\"]\n",
        "        clf.fit(X_train_fused, y_train)\n",
        "        y_pred = clf.predict(X_test_fused)\n",
        "        predictions[clf_name] =  y_pred\n",
        "        probabilities[clf_name] = clf.predict_proba(X_test_fused)[:, 1]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Calculate various evaluation metrics\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        precision = precision_score(y_test, y_pred, average='weighted', zero_division=1)\n",
        "        recall = recall_score(y_test, y_pred, average='weighted', zero_division=1)\n",
        "        f1 = f1_score(y_test, y_pred, average='weighted', zero_division=1)\n",
        "\n",
        "        # Store metrics in the results dictionary\n",
        "        results[f\"{fusion_name} + {clf_name}\"] = {\n",
        "            \"accuracy\": accuracy,\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"f1\": f1\n",
        "        }\n",
        "\n",
        "        # Identify and print misclassified samples\n",
        "        misclassified_indices = np.where(y_test != y_pred)[0]\n",
        "        if len(misclassified_indices) > 0:\n",
        "            print(f\"\\nMisclassified samples for {fusion_name} + {clf_name}:\")\n",
        "            for idx in misclassified_indices:\n",
        "                print(f\"  Index: {idx}, True Label: {y_test[idx]}, Predicted Label: {y_pred[idx]}\")\n",
        "    pred_df = pd.DataFrame(predictions)\n",
        "\n",
        "    weighted_votes = np.zeros(len(X_test_fused))\n",
        "\n",
        "    for name, weight in weights.items():\n",
        "        weighted_votes += pred_df[name] * weight\n",
        "    majority_vote_pred = pred_df.mode(axis=1)[0]  # Most common prediction for each instance\n",
        "\n",
        "    weighted_vote_pred = (weighted_votes >= (sum(weights.values()) / 2)).astype(int)\n",
        "    avg_prob = sum(weight * probabilities[name] for name, weight in weights.items()) / sum(weights.values())\n",
        "    prob_avg_pred = (avg_prob >= 0.5).astype(int)\n",
        "    accuracy, precision, recall,f1 =  evaluate(y_test, majority_vote_pred)\n",
        "    results[f\"{fusion_name} + Majority Voting\"] = {\n",
        "            \"accuracy\": accuracy,\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"f1\": f1\n",
        "    }\n",
        "    accuracy, precision, recall,f1 =  evaluate(y_test, weighted_vote_pred)\n",
        "    results[f\"{fusion_name} + Weighted Voting\"] = {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1\n",
        "    }\n",
        "    accuracy, precision, recall,f1 =  evaluate(y_test, prob_avg_pred)\n",
        "    results[f\"{fusion_name} + Probability Voting\"] = {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1\n",
        "    }\n",
        "\n",
        "\n",
        "# Define and train a simple MLP model\n",
        "mlp_model = Sequential()\n",
        "mlp_model.add(Dense(5, input_dim=X_train_weighted.shape[1], activation='relu'))\n",
        "mlp_model.add(Dense(5, activation='relu'))\n",
        "mlp_model.add(Dense(len(weights), activation='softmax'))\n",
        "print(len(weights))\n",
        "\n",
        "mlp_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# Set up K-Fold cross-validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=6)\n",
        "for steps in range(5):\n",
        "    for fold, (train_index, test_index) in enumerate(kf.split(X_train_weighted)):\n",
        "        print(f\"\\nProcessing Fold {fold + 1}\")\n",
        "\n",
        "        # Split data into train and test sets for the current fold\n",
        "        X_train, X_val = X_train_weighted[train_index], X_train_weighted[test_index]\n",
        "        y_train_mlp, y_val = y_train[train_index], y_train[test_index]\n",
        "\n",
        "        _classifiers ={\n",
        "            \"Random Forest\": RandomForestClassifier(random_state=42),\n",
        "            \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "            \"XGBoost\": XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
        "            \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
        "            \"SVM\": SVC(probability=True, random_state=42),\n",
        "            \"KNN\": KNeighborsClassifier(),\n",
        "            \"Naive Bayes\": GaussianNB(),\n",
        "            \"Gradient Boost\": GradientBoostingClassifier(random_state=42),\n",
        "        }\n",
        "\n",
        "        # Train and evaluate each classifier\n",
        "        for clf_name, clf in _classifiers.items():\n",
        "            clf.fit(X_train, y_train_mlp)\n",
        "            # y_pred = clf.predict_proba(X_test_fused)\n",
        "\n",
        "        y_for_mlp = np.zeros((X_val.shape[0],len(_classifiers)))\n",
        "        for idx,_x in enumerate(X_val):\n",
        "            # print(_y.shape)\n",
        "            # print(np.argmax(_y,axis=0))\n",
        "            if y_val[idx] == 1:\n",
        "                _y = np.array([clf.predict_proba(_x.reshape(1,-1))*(results[f\"Weighted + {_}\"][\"f1\"])**2 for _,clf in _classifiers.items()])[:,0,1]\n",
        "\n",
        "            else:\n",
        "                _y = np.array([clf.predict_proba(_x.reshape(1,-1))*(results[f\"Weighted + {_}\"][\"f1\"])**2 for _,clf in _classifiers.items()])[:,0,0]\n",
        "            y_for_mlp[idx,np.argmax(_y)] = 1\n",
        "        y_for_mlp = np.argmax(y_for_mlp,axis=1)\n",
        "        print(y_for_mlp)\n",
        "        mlp_model.fit(X_val, y_for_mlp, epochs=50, batch_size=10, verbose=1)\n",
        "\n",
        "selected_classifier = np.argmax(mlp_model.predict(X_test_weighted), axis=1)\n",
        "_y_pred = np.zeros((y_test.shape[0],len(_classifiers)))\n",
        "\n",
        "for i, clf_name in enumerate(weights.keys()):\n",
        "    clf = classifiers[f\"Weighted {clf_name}\"]\n",
        "    _y_pred[:,i] = clf.predict(X_test_weighted)\n",
        "# y_pred = _y_pred[:,selected_classifier]\n",
        "y_pred = _y_pred[np.arange(_y_pred.shape[0]), selected_classifier]\n",
        "print(selected_classifier)\n",
        "print(selected_classifier.shape)\n",
        "print(y_pred)\n",
        "print(y_pred.shape)\n",
        "# Calculate various evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted', zero_division=1)\n",
        "recall = recall_score(y_test, y_pred, average='weighted', zero_division=1)\n",
        "f1 = f1_score(y_test, y_pred, average='weighted', zero_division=1)\n",
        "\n",
        "# Store metrics in the results dictionary\n",
        "MCF_results = {\n",
        "    \"accuracy\": accuracy,\n",
        "    \"precision\": precision,\n",
        "    \"recall\": recall,\n",
        "    \"f1\": f1\n",
        "}\n",
        "print(MCF_results)\n",
        "# Identify and print misclassified samples\n",
        "misclassified_indices = np.where(y_test != y_pred)[0]\n",
        "if len(misclassified_indices) > 0:\n",
        "    print(f\"\\nMisclassified samples for {fusion_name} + {clf_name}:\")\n",
        "    for idx in misclassified_indices:\n",
        "        print(f\"  Index: {idx}, True Label: {y_test[idx]}, Predicted Label: {y_pred[idx]}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "qIFLzPo4o7uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def create_performance_heatmap(results, title):\n",
        "    # Create a DataFrame from the results dictionary\n",
        "    metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
        "    data = []\n",
        "    for model, scores in results.items():\n",
        "        row = [scores[metric] for metric in metrics]\n",
        "        data.append(row)\n",
        "\n",
        "    df = pd.DataFrame(data, columns=metrics, index=results.keys())\n",
        "\n",
        "    # Create heatmap\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.heatmap(df, annot=True, cmap='YlOrRd', fmt='.3f', cbar_kws={'label': 'Score'})\n",
        "    plt.title(title)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def create_fusion_comparison(results):\n",
        "    # Prepare data for each fusion method\n",
        "    fusion_methods = ['PCA', 'Concatenation', 'Sparse', 'Weighted']\n",
        "    metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
        "\n",
        "    # Create DataFrame for easier manipulation\n",
        "    df = pd.DataFrame(results).T\n",
        "\n",
        "    # Calculate mean scores for each fusion method\n",
        "    fusion_means = {}\n",
        "    for fusion in fusion_methods:\n",
        "        # Filter rows containing the fusion method name\n",
        "        fusion_data = df[df.index.str.contains(fusion)]\n",
        "        fusion_means[fusion] = fusion_data.mean()\n",
        "\n",
        "    # Create grouped bar plot\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "    x = np.arange(len(fusion_methods))\n",
        "    width = 0.2\n",
        "\n",
        "    for i, metric in enumerate(metrics):\n",
        "        values = [fusion_means[fusion][metric] for fusion in fusion_methods]\n",
        "        ax.bar(x + i*width, values, width, label=metric.capitalize())\n",
        "\n",
        "    ax.set_ylabel('Score')\n",
        "    ax.set_title('Performance Comparison of Fusion Methods')\n",
        "    ax.set_xticks(x + width * 1.5)\n",
        "    ax.set_xticklabels(fusion_methods)\n",
        "    ax.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def create_classifier_comparison(results):\n",
        "    classifiers = ['Random Forest', 'XGBoost', 'Decision Tree', 'SVM', 'KNN',\n",
        "                  'Logistic Regression', 'Naive Bayes', 'Gradient Boost']\n",
        "    metrics = ['accuracy', 'f1']\n",
        "\n",
        "    # Create DataFrame for easier manipulation\n",
        "    df = pd.DataFrame(results).T\n",
        "\n",
        "    # Calculate mean scores for each classifier\n",
        "    classifier_means = {}\n",
        "    for clf in classifiers:\n",
        "        # Filter rows containing the classifier name\n",
        "        clf_data = df[df.index.str.contains(clf)]\n",
        "        classifier_means[clf] = clf_data.mean()\n",
        "\n",
        "    # Create grouped bar plot\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "    x = np.arange(len(classifiers))\n",
        "    width = 0.35\n",
        "\n",
        "    for i, metric in enumerate(metrics):\n",
        "        values = [classifier_means[clf][metric] for clf in classifiers]\n",
        "        ax.bar(x + i*width, values, width, label=metric.capitalize())\n",
        "\n",
        "    ax.set_ylabel('Score')\n",
        "    ax.set_title('Performance Comparison of Classifiers')\n",
        "    ax.set_xticks(x + width/2)\n",
        "    ax.set_xticklabels(classifiers, rotation=45, ha='right')\n",
        "    ax.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def visualize_mcf_comparison(results, mcf_results):\n",
        "    # Prepare data for visualization\n",
        "    methods = ['Best Traditional', 'MCF']\n",
        "    metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
        "\n",
        "    # Find best traditional method\n",
        "    best_traditional = max(results.items(), key=lambda x: x[1]['f1'])\n",
        "\n",
        "    # Create comparison data\n",
        "    comparison_data = {\n",
        "        'Best Traditional': best_traditional[1],\n",
        "        'MCF': mcf_results\n",
        "    }\n",
        "\n",
        "    # Create bar plot\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    x = np.arange(len(methods))\n",
        "    width = 0.2\n",
        "\n",
        "    for i, metric in enumerate(metrics):\n",
        "        values = [comparison_data[method][metric] for method in methods]\n",
        "        ax.bar(x + i*width, values, width, label=metric.capitalize())\n",
        "\n",
        "    ax.set_ylabel('Score')\n",
        "    ax.set_title('MCF vs Best Traditional Method')\n",
        "    ax.set_xticks(x + width * 1.5)\n",
        "    ax.set_xticklabels(methods)\n",
        "    ax.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Let's also create a summary function to print the best performers\n",
        "def print_summary(results, mcf_results):\n",
        "    print(\"=== Performance Summary ===\")\n",
        "\n",
        "    # Find best model for each metric\n",
        "    metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
        "    for metric in metrics:\n",
        "        best_model = max(results.items(), key=lambda x: x[1][metric])\n",
        "        print(f\"\\nBest {metric}: {best_model[0]}\")\n",
        "        print(f\"Score: {best_model[1][metric]:.4f}\")\n",
        "\n",
        "    print(\"\\n=== MCF Performance ===\")\n",
        "    for metric in metrics:\n",
        "        print(f\"{metric}: {mcf_results[metric]:.4f}\")\n",
        "\n",
        "# Create all visualizations\n",
        "create_performance_heatmap(results, 'Performance Metrics Heatmap')\n",
        "create_fusion_comparison(results)\n",
        "create_classifier_comparison(results)\n",
        "visualize_mcf_comparison(results, MCF_results)\n",
        "print_summary(results, MCF_results)"
      ],
      "metadata": {
        "id": "b-RQbHci5CBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "def plot_confusion_matrices(results, y_test, mcf_pred, class_names=None):\n",
        "    # Find best traditional model based on F1 score\n",
        "    best_traditional = max(results.items(), key=lambda x: x[1]['f1'])\n",
        "    best_model_name = best_traditional[0]\n",
        "\n",
        "    # Get predictions for best traditional model\n",
        "    for fusion_name, (X_train_fused, X_test_fused) in fused_datasets.items():\n",
        "        if fusion_name in best_model_name:\n",
        "            for clf_name, clf in classifiers.items():\n",
        "                if clf_name.endswith(best_model_name.split('+')[1].strip()):\n",
        "                    best_traditional_pred = clf.predict(X_test_fused)\n",
        "                    break\n",
        "            break\n",
        "\n",
        "    # Create figure with two subplots\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "    # Plot confusion matrix for best traditional model\n",
        "    cm1 = confusion_matrix(y_test, best_traditional_pred)\n",
        "    sns.heatmap(cm1, annot=True, fmt='d', cmap='Blues', ax=ax1)\n",
        "    ax1.set_title(f'Confusion Matrix\\nBest Traditional Model ({best_model_name})')\n",
        "    ax1.set_xlabel('Predicted')\n",
        "    ax1.set_ylabel('True')\n",
        "    if class_names:\n",
        "        ax1.set_xticklabels(class_names)\n",
        "        ax1.set_yticklabels(class_names)\n",
        "\n",
        "    # Plot confusion matrix for MCF model\n",
        "    cm2 = confusion_matrix(y_test, mcf_pred)\n",
        "    sns.heatmap(cm2, annot=True, fmt='d', cmap='Blues', ax=ax2)\n",
        "    ax2.set_title('Confusion Matrix\\nMCF Model')\n",
        "    ax2.set_xlabel('Predicted')\n",
        "    ax2.set_ylabel('True')\n",
        "    if class_names:\n",
        "        ax2.set_xticklabels(class_names)\n",
        "        ax2.set_yticklabels(class_names)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print additional metrics\n",
        "    print(\"\\n=== Confusion Matrix Analysis ===\")\n",
        "\n",
        "    def print_metrics_from_cm(cm, model_name):\n",
        "        tn, fp, fn, tp = cm.ravel()\n",
        "        specificity = tn / (tn + fp)\n",
        "        sensitivity = tp / (tp + fn)\n",
        "        print(f\"\\n{model_name} Metrics:\")\n",
        "        print(f\"True Negatives: {tn}\")\n",
        "        print(f\"False Positives: {fp}\")\n",
        "        print(f\"False Negatives: {fn}\")\n",
        "        print(f\"True Positives: {tp}\")\n",
        "        print(f\"Specificity (True Negative Rate): {specificity:.4f}\")\n",
        "        print(f\"Sensitivity (True Positive Rate): {sensitivity:.4f}\")\n",
        "\n",
        "    print_metrics_from_cm(cm1, \"Best Traditional Model\")\n",
        "    print_metrics_from_cm(cm2, \"MCF Model\")\n",
        "\n",
        "# Plot confusion matrices\n",
        "class_names = ['Class 0', 'Class 1']  # Update these names based on your actual classes\n",
        "plot_confusion_matrices(results, y_test, y_pred, class_names)"
      ],
      "metadata": {
        "id": "Od2DRLHgAI0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resualts"
      ],
      "metadata": {
        "id": "4su5tVkFF9H9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ucimlrepo\n",
        "\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.decomposition import SparsePCA\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "##\n",
        "\n",
        "\n",
        "# Assume that fetch_ucirepo and necessary dataset fetching utilities are defined\n",
        "# Fetch dataset\n",
        "darwin = fetch_ucirepo(id=732)\n",
        "\n",
        "# Data\n",
        "X = darwin.data.features.iloc[:, 1:]  # Removing the first feature as it is the ID of participant\n",
        "y = darwin.data.targets.iloc[:, 0]  # Assuming y is single-column\n",
        "\n",
        "# Convert categorical columns to numerical\n",
        "non_numeric_cols = X.select_dtypes(include=['object']).columns\n",
        "if len(non_numeric_cols) > 0:\n",
        "    for col in non_numeric_cols:\n",
        "        le = LabelEncoder()\n",
        "        X[col] = le.fit_transform(X[col])\n",
        "\n",
        "# Encode target\n",
        "y = LabelEncoder().fit_transform(y) if y.dtype == 'object' else y\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=6)\n",
        "\n",
        "# Data augmentation (if necessary)\n",
        "if False:\n",
        "    target_size = 500\n",
        "    current_size = X_train.shape[0]\n",
        "\n",
        "    if current_size < target_size:\n",
        "        samples_needed = target_size - current_size\n",
        "        # Sample additional data from X_train with replacement\n",
        "        additional_X = X_train.sample(n=samples_needed, replace=True, random_state=42)\n",
        "        # Sample additional labels directly from y_train\n",
        "        additional_y = np.random.choice(y_train, size=samples_needed, replace=True)\n",
        "\n",
        "        # Concatenate the original and augmented data\n",
        "        X_train = pd.concat([X_train, additional_X], ignore_index=True)\n",
        "        y_train = np.concatenate([y_train, additional_y])\n",
        "\n",
        "# Preprocessing: Low variance filtering, feature selection, and optional PCA\n",
        "# Remove low variance features\n",
        "numeric_cols = X_train.select_dtypes(include=[np.number]).columns\n",
        "variances = X_train[numeric_cols].var()\n",
        "low_variance_mask = variances > 0\n",
        "selected_numeric_cols = numeric_cols[low_variance_mask]\n",
        "\n",
        "# Ensure that non-numeric columns exist in X_train for union operation\n",
        "valid_non_numeric_cols = [col for col in non_numeric_cols if col in X_train.columns]\n",
        "\n",
        "# Filter train and test sets\n",
        "X_train_filtered = X_train[selected_numeric_cols.union(valid_non_numeric_cols)]\n",
        "X_test_filtered = X_test[selected_numeric_cols.union(valid_non_numeric_cols)]\n",
        "\n",
        "print(\"X_train_filtered shape:\", X_train_filtered.shape)\n",
        "print(\"X_test_filtered shape:\", X_test_filtered.shape)\n",
        "\n",
        "\n",
        "\n",
        "# Remove highly correlated features\n",
        "correlation_matrix = X_train_filtered.corr()\n",
        "correlation_threshold = 0.80\n",
        "upper_triangle = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
        "correlated_features = [column for column in upper_triangle.columns if any(upper_triangle[column] > correlation_threshold)]\n",
        "X_train_filtered = X_train_filtered.loc[:, ~X_train_filtered.columns.isin(correlated_features)]\n",
        "X_test_filtered = X_test_filtered.loc[:, ~X_test_filtered.columns.isin(correlated_features)]\n",
        "\n",
        "# Feature Selection: Use SelectKBest to select top features based on ANOVA F-value\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "select_kbest = SelectKBest(score_func=f_classif, k=20)  # Select top 20 features\n",
        "X_train_filtered = pd.DataFrame(select_kbest.fit_transform(X_train_filtered, y_train), columns=[f'feature_{i}' for i in range(20)])\n",
        "X_test_filtered = pd.DataFrame(select_kbest.transform(X_test_filtered), columns=[f'feature_{i}' for i in range(20)])\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_filtered)\n",
        "X_test_scaled = scaler.transform(X_test_filtered)\n",
        "\n",
        "# Apply PCA to reduce dimensions and retain 80% of variance\n",
        "pca = PCA(n_components=0.80)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "# Concatenation Fusion: Combine original filtered features with PCA features\n",
        "X_train_concat = np.concatenate((X_train_scaled, X_train_pca), axis=1)\n",
        "X_test_concat = np.concatenate((X_test_scaled, X_test_pca), axis=1)\n",
        "\n",
        "# Sparse Representation Fusion\n",
        "sparse_pca = SparsePCA(n_components=10, random_state=42)\n",
        "X_train_sparse = sparse_pca.fit_transform(X_train_scaled)\n",
        "X_test_sparse = sparse_pca.transform(X_test_scaled)\n",
        "\n",
        "# Weighted Fusion: Assign weights to PCA and original features\n",
        "weight_pca = 0.7\n",
        "weight_original = 0.3\n",
        "X_train_weighted = np.hstack((weight_original * X_train_scaled, weight_pca * X_train_pca))\n",
        "X_test_weighted = np.hstack((weight_original * X_test_scaled, weight_pca * X_test_pca))\n",
        "\n",
        "# Prepare traditional classifiers with different fused datasets\n",
        "fused_datasets = {\n",
        "    \"PCA\": (X_train_pca, X_test_pca),\n",
        "    \"Concatenation\": (X_train_concat, X_test_concat),\n",
        "    \"Sparse\": (X_train_sparse, X_test_sparse),\n",
        "    \"Weighted\": (X_train_weighted, X_test_weighted)\n",
        "}\n",
        "classifiers = {}\n",
        "\n",
        "for feature_name in fused_datasets.keys():\n",
        "    classifiers.update({\n",
        "        f\"{feature_name} Random Forest\": RandomForestClassifier(random_state=42),\n",
        "        f\"{feature_name} XGBoost\": XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
        "        f\"{feature_name} Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
        "        f\"{feature_name} SVM\": SVC(probability=True, random_state=42),\n",
        "        f\"{feature_name} KNN\": KNeighborsClassifier(),\n",
        "        f\"{feature_name} Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "        f\"{feature_name} Naive Bayes\": GaussianNB(),\n",
        "        f\"{feature_name} Gradient Boost\": GradientBoostingClassifier(random_state=42),\n",
        "    })\n",
        "\n",
        "# Additional methods (majority voting, weighted voting, probability averaging)\n",
        "weights = {\n",
        "    \"Random Forest\": 2,\n",
        "    \"Logistic Regression\": 1.5,\n",
        "    \"XGBoost\": 2,\n",
        "    \"Decision Tree\": 1,\n",
        "    \"SVM\": 1,\n",
        "    \"KNN\": 1,\n",
        "    \"Naive Bayes\": 1,\n",
        "    \"Gradient Boost\": 1,\n",
        "}\n",
        "\n",
        "results = {}\n",
        "predictions = {}\n",
        "probabilities = {}\n",
        "\n",
        "for fusion_name, (X_train_fused, X_test_fused) in fused_datasets.items():\n",
        "    print(f\"Number of features for {fusion_name} fusion: {X_train_fused.shape[1]}\")\n",
        "\n",
        "    for clf_name in weights.keys():\n",
        "        clf = classifiers[f\"{fusion_name} {clf_name}\"]\n",
        "        clf.fit(X_train_fused, y_train)\n",
        "        y_pred = clf.predict(X_test_fused)\n",
        "        predictions[clf_name] =  y_pred\n",
        "        probabilities[clf_name] = clf.predict_proba(X_test_fused)[:, 1]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Calculate various evaluation metrics\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        precision = precision_score(y_test, y_pred, average='weighted', zero_division=1)\n",
        "        recall = recall_score(y_test, y_pred, average='weighted', zero_division=1)\n",
        "        f1 = f1_score(y_test, y_pred, average='weighted', zero_division=1)\n",
        "\n",
        "        # Store metrics in the results dictionary\n",
        "        results[f\"{fusion_name} + {clf_name}\"] = {\n",
        "            \"accuracy\": accuracy,\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"f1\": f1\n",
        "        }\n",
        "\n",
        "        # Identify and print misclassified samples\n",
        "        misclassified_indices = np.where(y_test != y_pred)[0]\n",
        "        if len(misclassified_indices) > 0:\n",
        "            print(f\"\\nMisclassified samples for {fusion_name} + {clf_name}:\")\n",
        "            for idx in misclassified_indices:\n",
        "                print(f\"  Index: {idx}, True Label: {y_test[idx]}, Predicted Label: {y_pred[idx]}\")\n",
        "    pred_df = pd.DataFrame(predictions)\n",
        "\n",
        "    weighted_votes = np.zeros(len(X_test_fused))\n",
        "\n",
        "    for name, weight in weights.items():\n",
        "        weighted_votes += pred_df[name] * weight\n",
        "    majority_vote_pred = pred_df.mode(axis=1)[0]  # Most common prediction for each instance\n",
        "\n",
        "    weighted_vote_pred = (weighted_votes >= (sum(weights.values()) / 2)).astype(int)\n",
        "    avg_prob = sum(weight * probabilities[name] for name, weight in weights.items()) / sum(weights.values())\n",
        "    prob_avg_pred = (avg_prob >= 0.5).astype(int)\n",
        "    accuracy, precision, recall,f1 =  evaluate(y_test, majority_vote_pred)\n",
        "    results[f\"{fusion_name} + Majority Voting\"] = {\n",
        "            \"accuracy\": accuracy,\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"f1\": f1\n",
        "    }\n",
        "    accuracy, precision, recall,f1 =  evaluate(y_test, weighted_vote_pred)\n",
        "    results[f\"{fusion_name} + Weighted Voting\"] = {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1\n",
        "    }\n",
        "    accuracy, precision, recall,f1 =  evaluate(y_test, prob_avg_pred)\n",
        "    results[f\"{fusion_name} + Probability Voting\"] = {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1\n",
        "    }\n",
        "\n",
        "\n",
        "# Define and train a simple MLP model\n",
        "mlp_model = Sequential()\n",
        "mlp_model.add(Dense(3, input_dim=X_train_weighted.shape[1], activation='relu'))\n",
        "mlp_model.add(Dense(3, activation='relu'))\n",
        "mlp_model.add(Dense(len(weights), activation='softmax'))\n",
        "print(len(weights))\n",
        "\n",
        "mlp_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# Set up K-Fold cross-validation\n",
        "kf = KFold(n_splits=9, shuffle=True, random_state=6)\n",
        "for steps in range(5):\n",
        "    for fold, (train_index, test_index) in enumerate(kf.split(X_train_weighted)):\n",
        "        print(f\"\\nProcessing Fold {fold + 1}\")\n",
        "\n",
        "        # Split data into train and test sets for the current fold\n",
        "        X_train, X_val = X_train_weighted[train_index], X_train_weighted[test_index]\n",
        "        y_train_mlp, y_val = y_train[train_index], y_train[test_index]\n",
        "\n",
        "        _classifiers ={\n",
        "            \"Random Forest\": RandomForestClassifier(random_state=42),\n",
        "            \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "            \"XGBoost\": XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
        "            \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
        "            \"SVM\": SVC(probability=True, random_state=42),\n",
        "            \"KNN\": KNeighborsClassifier(),\n",
        "            \"Naive Bayes\": GaussianNB(),\n",
        "            \"Gradient Boost\": GradientBoostingClassifier(random_state=42),\n",
        "        }\n",
        "\n",
        "        # Train and evaluate each classifier\n",
        "        for clf_name, clf in _classifiers.items():\n",
        "            clf.fit(X_train, y_train_mlp)\n",
        "            # y_pred = clf.predict_proba(X_test_fused)\n",
        "\n",
        "        y_for_mlp = np.zeros((X_val.shape[0],len(_classifiers)))\n",
        "        for idx,_x in enumerate(X_val):\n",
        "            # print(_y.shape)\n",
        "            # print(np.argmax(_y,axis=0))\n",
        "            if y_val[idx] == 1:\n",
        "                _y = np.array([clf.predict_proba(_x.reshape(1,-1))*(results[f\"Weighted + {_}\"][\"f1\"]) for _,clf in _classifiers.items()])[:,0,1]\n",
        "\n",
        "            else:\n",
        "                _y = np.array([clf.predict_proba(_x.reshape(1,-1))*(results[f\"Weighted + {_}\"][\"f1\"]) for _,clf in _classifiers.items()])[:,0,0]\n",
        "            y_for_mlp[idx,np.argmax(_y)] = 1\n",
        "        y_for_mlp = np.argmax(y_for_mlp,axis=1)\n",
        "        print(y_for_mlp)\n",
        "        mlp_model.fit(X_val, y_for_mlp, epochs=50, batch_size=10, verbose=1)\n",
        "\n",
        "selected_classifier = np.argmax(mlp_model.predict(X_test_weighted), axis=1)\n",
        "_y_pred = np.zeros((y_test.shape[0],len(_classifiers)))\n",
        "\n",
        "for i, clf_name in enumerate(weights.keys()):\n",
        "    clf = classifiers[f\"Weighted {clf_name}\"]\n",
        "    _y_pred[:,i] = clf.predict(X_test_weighted)\n",
        "# y_pred = _y_pred[:,selected_classifier]\n",
        "y_pred = _y_pred[np.arange(_y_pred.shape[0]), selected_classifier]\n",
        "print(selected_classifier)\n",
        "print(selected_classifier.shape)\n",
        "print(y_pred)\n",
        "print(y_pred.shape)\n",
        "# Calculate various evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted', zero_division=1)\n",
        "recall = recall_score(y_test, y_pred, average='weighted', zero_division=1)\n",
        "f1 = f1_score(y_test, y_pred, average='weighted', zero_division=1)\n",
        "\n",
        "# Store metrics in the results dictionary\n",
        "MCF_results = {\n",
        "    \"accuracy\": accuracy,\n",
        "    \"precision\": precision,\n",
        "    \"recall\": recall,\n",
        "    \"f1\": f1\n",
        "}\n",
        "print(MCF_results)\n",
        "# Identify and print misclassified samples\n",
        "misclassified_indices = np.where(y_test != y_pred)[0]\n",
        "if len(misclassified_indices) > 0:\n",
        "    print(f\"\\nMisclassified samples for {fusion_name} + {clf_name}:\")\n",
        "    for idx in misclassified_indices:\n",
        "        print(f\"  Index: {idx}, True Label: {y_test[idx]}, Predicted Label: {y_pred[idx]}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "REWTh7f0Kpyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import pandas as pd\n",
        "\n",
        "def get_all_predictions():\n",
        "    predictions = {}\n",
        "    # Get predictions for all models\n",
        "    j=0\n",
        "    for fusion_name, (X_train_fused, X_test_fused) in fused_datasets.items():\n",
        "        for clf_name, clf in classifiers.items():\n",
        "            if clf_name.startswith(fusion_name):\n",
        "                model_name = f\"{fusion_name} + {clf_name.split(' ', 1)[1]}\"\n",
        "                predictions[model_name] = clf.predict(X_test_fused)\n",
        "\n",
        "        # Add ensemble methods predictions\n",
        "        weighted_votes = np.zeros(len(X_test_fused))\n",
        "        for name, weight in weights.items():\n",
        "            weighted_votes += predictions[f\"{fusion_name} + {name}\"] * weight\n",
        "        predictions[f\"{fusion_name} + Weighted Voting\"] = (weighted_votes >= (sum(weights.values()) / 2)).astype(int)\n",
        "\n",
        "        # Majority voting\n",
        "        pred_df = pd.DataFrame({name: predictions[f\"{fusion_name} + {name}\"] for name in weights.keys()})\n",
        "        predictions[f\"{fusion_name} + Majority Voting\"] = pred_df.mode(axis=1)[0]\n",
        "\n",
        "    # Add MCF predictions\n",
        "        predictions[f\"{fusion_name} + MCF\"] = y_predict_MCF[:,j].squeeze()\n",
        "        j += 1\n",
        "\n",
        "    return predictions\n",
        "\n",
        "\n",
        "def create_comprehensive_metrics_table(predictions):\n",
        "    metrics_data = []\n",
        "\n",
        "    for model_name, y_pred in predictions.items():\n",
        "        # Calculate confusion matrix\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "        tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "        # Calculate primary metrics\n",
        "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "        recall = sensitivity  # Sensitivity is another term for recall\n",
        "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "        # Calculate F2 score, which weights recall (sensitivity) higher than precision\n",
        "        f2 = (1 + 2**2) * (precision * recall) / ((2**2 * precision) + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "        metrics_data.append({\n",
        "            'Model': model_name,\n",
        "            'Accuracy': accuracy,\n",
        "            'Precision': precision,\n",
        "            'Recall (Sensitivity)': recall,\n",
        "            'F1 Score': f1,\n",
        "            'F2 Score': f2,  # Emphasis on recall, minimizing false negatives\n",
        "            'Specificity': specificity,\n",
        "            'True Negatives (TN)': tn,\n",
        "            'False Positives (FP)': fp,\n",
        "            'False Negatives (FN)': fn,\n",
        "            'True Positives (TP)': tp\n",
        "        })\n",
        "\n",
        "    # Create DataFrame\n",
        "    metrics_df = pd.DataFrame(metrics_data)\n",
        "\n",
        "    # Sort primarily by F2 Score and F1 Score to prioritize models minimizing FN\n",
        "    metrics_df = metrics_df.sort_values(['F2 Score', 'F1 Score'], ascending=False)\n",
        "\n",
        "    return metrics_df\n",
        "\n",
        "def plot_all_confusion_matrices(predictions):\n",
        "    n_models = len(predictions)\n",
        "    n_cols = 4\n",
        "    n_rows = (n_models + n_cols - 1) // n_cols\n",
        "\n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 5*n_rows))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i, (model_name, y_pred) in enumerate(predictions.items()):\n",
        "        if i < len(axes):\n",
        "            cm = confusion_matrix(y_test, y_pred)\n",
        "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i])\n",
        "            axes[i].set_title(f'Confusion Matrix\\n{model_name}')\n",
        "            axes[i].set_xlabel('Predicted')\n",
        "            axes[i].set_ylabel('True')\n",
        "\n",
        "    # Remove empty subplots\n",
        "    for i in range(len(predictions), len(axes)):\n",
        "        fig.delaxes(axes[i])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_performance_comparison(metrics_df):\n",
        "    # Select metrics to plot\n",
        "    plot_metrics = ['Accuracy', 'Precision', 'Recall', 'F1']\n",
        "\n",
        "    # Create figure\n",
        "    plt.figure(figsize=(15, 8))\n",
        "\n",
        "    # Plot\n",
        "    x = np.arange(len(metrics_df))\n",
        "    width = 0.2\n",
        "\n",
        "    for i, metric in enumerate(plot_metrics):\n",
        "        plt.bar(x + i*width, metrics_df[metric], width, label=metric)\n",
        "\n",
        "    plt.xlabel('Models')\n",
        "    plt.ylabel('Score')\n",
        "    plt.title('Performance Comparison Across All Models')\n",
        "    plt.xticks(x + width*1.5, metrics_df['Model'], rotation=45, ha='right')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Get all predictions\n",
        "all_predictions = get_all_predictions()\n",
        "\n",
        "# Create and display metrics table\n",
        "metrics_df = create_comprehensive_metrics_table(all_predictions )\n",
        "print(\"\\n=== Comprehensive Metrics Table ===\")\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "print(metrics_df.to_string(index=False))\n",
        "\n",
        "# Plot confusion matrices\n",
        "print(\"\\n=== Confusion Matrices for All Models ===\")\n",
        "plot_all_confusion_matrices(all_predictions)\n",
        "\n",
        "# Plot performance comparison\n",
        "print(\"\\n=== Performance Comparison ===\")\n",
        "plot_performance_comparison(metrics_df)\n",
        "\n",
        "# Export metrics to CSV (optional)\n",
        "metrics_df.to_csv('model_metrics.csv', index=False)"
      ],
      "metadata": {
        "id": "7Y7OPQUhBFeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import pandas as pd\n",
        "\n",
        "def get_all_predictions():\n",
        "    predictions = {}\n",
        "    # Get predictions for all models\n",
        "    j=0\n",
        "    for fusion_name, (X_train_fused, X_test_fused) in fused_datasets.items():\n",
        "        for clf_name, clf in classifiers.items():\n",
        "            if clf_name.startswith(fusion_name):\n",
        "                model_name = f\"{fusion_name} + {clf_name.split(' ', 1)[1]}\"\n",
        "                predictions[model_name] = clf.predict(X_test_fused)\n",
        "\n",
        "        # Add ensemble methods predictions\n",
        "        weighted_votes = np.zeros(len(X_test_fused))\n",
        "        for name, weight in weights.items():\n",
        "            weighted_votes += predictions[f\"{fusion_name} + {name}\"] * weight\n",
        "        predictions[f\"{fusion_name} + Weighted Voting\"] = (weighted_votes >= (sum(weights.values()) / 2)).astype(int)\n",
        "\n",
        "        # Majority voting\n",
        "        pred_df = pd.DataFrame({name: predictions[f\"{fusion_name} + {name}\"] for name in weights.keys()})\n",
        "        predictions[f\"{fusion_name} + Majority Voting\"] = pred_df.mode(axis=1)[0]\n",
        "\n",
        "    # Add MCF predictions\n",
        "        predictions[f\"{fusion_name} + MCF\"] = y_predict_MCF[:,j].squeeze()\n",
        "        j += 1\n",
        "\n",
        "    return predictions\n",
        "\n",
        "\n",
        "def create_comprehensive_metrics_table(predictions):\n",
        "    metrics_data = []\n",
        "\n",
        "    for model_name, y_pred in predictions.items():\n",
        "        # Calculate confusion matrix\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "        tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "        # Calculate primary metrics\n",
        "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "        recall = sensitivity  # Sensitivity is another term for recall\n",
        "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "        # Calculate F2 score, which weights recall (sensitivity) higher than precision\n",
        "        f2 = (1 + 2**2) * (precision * recall) / ((2**2 * precision) + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "        metrics_data.append({\n",
        "            'Model': model_name,\n",
        "            'Accuracy': accuracy,\n",
        "            'Precision': precision,\n",
        "            'Recall (Sensitivity)': recall,\n",
        "            'F1 Score': f1,\n",
        "            'F2 Score': f2,  # Emphasis on recall, minimizing false negatives\n",
        "            'Specificity': specificity,\n",
        "            'True Negatives (TN)': tn,\n",
        "            'False Positives (FP)': fp,\n",
        "            'False Negatives (FN)': fn,\n",
        "            'True Positives (TP)': tp\n",
        "        })\n",
        "\n",
        "    # Create DataFrame\n",
        "    metrics_df = pd.DataFrame(metrics_data)\n",
        "\n",
        "    # Sort primarily by F2 Score and F1 Score to prioritize models minimizing FN\n",
        "    metrics_df = metrics_df.sort_values(['F2 Score', 'F1 Score'], ascending=False)\n",
        "\n",
        "    return metrics_df\n",
        "\n",
        "def plot_all_confusion_matrices(predictions):\n",
        "    n_models = len(predictions)\n",
        "    n_cols = 4\n",
        "    n_rows = (n_models + n_cols - 1) // n_cols\n",
        "\n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 5*n_rows))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i, (model_name, y_pred) in enumerate(predictions.items()):\n",
        "        if i < len(axes):\n",
        "            cm = confusion_matrix(y_test, y_pred)\n",
        "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i])\n",
        "            axes[i].set_title(f'Confusion Matrix\\n{model_name}')\n",
        "            axes[i].set_xlabel('Predicted')\n",
        "            axes[i].set_ylabel('True')\n",
        "\n",
        "    # Remove empty subplots\n",
        "    for i in range(len(predictions), len(axes)):\n",
        "        fig.delaxes(axes[i])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_performance_comparison(metrics_df):\n",
        "    # Select metrics to plot\n",
        "    plot_metrics = ['Accuracy', 'Precision', 'Recall', 'F1']\n",
        "\n",
        "    # Create figure\n",
        "    plt.figure(figsize=(15, 8))\n",
        "\n",
        "    # Plot\n",
        "    x = np.arange(len(metrics_df))\n",
        "    width = 0.2\n",
        "\n",
        "    for i, metric in enumerate(plot_metrics):\n",
        "        plt.bar(x + i*width, metrics_df[metric], width, label=metric)\n",
        "\n",
        "    plt.xlabel('Models')\n",
        "    plt.ylabel('Score')\n",
        "    plt.title('Performance Comparison Across All Models')\n",
        "    plt.xticks(x + width*1.5, metrics_df['Model'], rotation=45, ha='right')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Get all predictions\n",
        "all_predictions = get_all_predictions()\n",
        "\n",
        "# Create and display metrics table\n",
        "metrics_df = create_comprehensive_metrics_table(all_predictions )\n",
        "print(\"\\n=== Comprehensive Metrics Table ===\")\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "print(metrics_df.to_string(index=False))\n",
        "\n",
        "# Plot confusion matrices\n",
        "print(\"\\n=== Confusion Matrices for All Models ===\")\n",
        "plot_all_confusion_matrices(all_predictions)\n",
        "\n",
        "# Plot performance comparison\n",
        "print(\"\\n=== Performance Comparison ===\")\n",
        "plot_performance_comparison(metrics_df)\n",
        "\n",
        "# Export metrics to CSV (optional)\n",
        "metrics_df.to_csv('model_metrics.csv', index=False)"
      ],
      "metadata": {
        "id": "Ws32GBtXVNW3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}